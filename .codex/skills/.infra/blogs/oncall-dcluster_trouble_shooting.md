# DCluster Spark Job æ•…éšœæ’æŸ¥æŒ‡å—

## å¿«é€Ÿå‚è€ƒ

### ğŸš¨ ç´§æ€¥è”ç³»äºº
- **ä¸»è¦è”ç³»äºº**: @Junhan Ouyang
- **é—®é¢˜ç±»å‹**: DCluster Spark Job ç›¸å…³é—®é¢˜

## å¸¸è§é—®é¢˜

### 1. Spark Job çŠ¶æ€å¼‚å¸¸

#### é—®é¢˜ç°è±¡
```
Spark Job Status: waiting for spark master ready
```

#### é—®é¢˜æè¿°
Spark Job ä¸€ç›´å¤„äºç­‰å¾… Spark Master å‡†å¤‡å°±ç»ªçš„çŠ¶æ€ï¼Œæ— æ³•æ­£å¸¸å¯åŠ¨ã€‚

#### å¯èƒ½åŸå› 
- Spark Master æœåŠ¡æœªæ­£å¸¸å¯åŠ¨
- é›†ç¾¤èµ„æºä¸è¶³
- ç½‘ç»œè¿æ¥é—®é¢˜
- é…ç½®é—®é¢˜

#### è§£å†³æ–¹æ¡ˆ

**ä¸´æ—¶è§£å†³æ–¹æ¡ˆ**ï¼š
1. **å¢åŠ ç§Ÿæˆ·ä¿¡æ¯ Worker æ•°é‡**
   ```bash
   # å°† tenant info worker æ•°é‡å¢åŠ åˆ° 60 ä»¥ä¸Š
   # è¿™é€šå¸¸å¯ä»¥è§£å†³ Spark Master å‡†å¤‡å°±ç»ªçš„é—®é¢˜
   ```

2. **æ£€æŸ¥ Spark Master çŠ¶æ€**
   ```bash
   # æ£€æŸ¥ Spark Master Pod çŠ¶æ€
   kubectl get pods -n dcluster | grep spark-master
   
   # æŸ¥çœ‹ Spark Master æ—¥å¿—
   kubectl logs -n dcluster <spark-master-pod-name>
   ```

3. **æ£€æŸ¥é›†ç¾¤èµ„æº**
   ```bash
   # æ£€æŸ¥èŠ‚ç‚¹èµ„æºä½¿ç”¨æƒ…å†µ
   kubectl top nodes
   
   # æ£€æŸ¥ Pod èµ„æºä½¿ç”¨æƒ…å†µ
   kubectl top pods -n dcluster
   ```

**é•¿æœŸè§£å†³æ–¹æ¡ˆ**ï¼š
- ä¼˜åŒ– Spark é›†ç¾¤é…ç½®
- å¢åŠ é›†ç¾¤èµ„æºå®¹é‡
- å®ç°è‡ªåŠ¨æ‰©ç¼©å®¹æœºåˆ¶

#### é¢„æœŸè§£å†³æ—¶é—´
- **ä¸´æ—¶æ–¹æ¡ˆ**: ç«‹å³ç”Ÿæ•ˆ
- **æ ¹æœ¬è§£å†³**: é¢„è®¡å¾ˆå¿«ä¼šå¾—åˆ°è§£å†³

## æ’æŸ¥æ­¥éª¤

### 1. åˆæ­¥è¯Šæ–­
1. **æ£€æŸ¥ Spark Job çŠ¶æ€**
   ```bash
   kubectl get sparkapplications -n dcluster
   ```

2. **æŸ¥çœ‹ Job è¯¦ç»†ä¿¡æ¯**
   ```bash
   kubectl describe sparkapplication <job-name> -n dcluster
   ```

3. **æ£€æŸ¥ç›¸å…³ Pod çŠ¶æ€**
   ```bash
   kubectl get pods -n dcluster | grep spark
   ```

### 2. æ·±å…¥åˆ†æ
1. **æŸ¥çœ‹ Spark Master æ—¥å¿—**
   ```bash
   kubectl logs -n dcluster <spark-master-pod-name> --tail=100
   ```

2. **æ£€æŸ¥ Worker èŠ‚ç‚¹çŠ¶æ€**
   ```bash
   kubectl get pods -n dcluster | grep worker
   ```

3. **æ£€æŸ¥èµ„æºé…é¢**
   ```bash
   kubectl describe resourcequota -n dcluster
   ```

### 3. ç½‘ç»œè¯Šæ–­
1. **æ£€æŸ¥æœåŠ¡è¿é€šæ€§**
   ```bash
   kubectl get svc -n dcluster | grep spark
   ```

2. **æµ‹è¯•ç½‘ç»œè¿æ¥**
   ```bash
   kubectl exec -n dcluster <pod-name> -- curl -v <spark-master-service>
   ```

## é¢„é˜²æªæ–½

### 1. ç›‘æ§è®¾ç½®
- è®¾ç½® Spark Master å¥åº·æ£€æŸ¥ç›‘æ§
- ç›‘æ§é›†ç¾¤èµ„æºä½¿ç”¨ç‡
- è®¾ç½® Job æ‰§è¡Œæ—¶é—´å‘Šè­¦

### 2. é…ç½®ä¼˜åŒ–
- åˆç†é…ç½® Spark èµ„æºå‚æ•°
- è®¾ç½®åˆé€‚çš„è¶…æ—¶æ—¶é—´
- ä¼˜åŒ–å†…å­˜å’Œ CPU åˆ†é…

### 3. æ–‡æ¡£ç»´æŠ¤
- è®°å½•å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ
- æ›´æ–°æ•…éšœå¤„ç†æ‰‹å†Œ
- å®šæœŸå›é¡¾å’Œä¼˜åŒ–æµç¨‹

## ç›¸å…³èµ„æº

### 1. å®˜æ–¹æ–‡æ¡£
- [Spark on Kubernetes](https://spark.apache.org/docs/latest/running-on-kubernetes.html)
- [Kubernetes Spark Operator](https://github.com/GoogleCloudPlatform/spark-on-k8s-operator)

### 2. å†…éƒ¨èµ„æº
- DCluster é…ç½®æ–‡æ¡£
- Spark é›†ç¾¤ç®¡ç†æ‰‹å†Œ
- æ•…éšœå¤„ç†æµç¨‹æ–‡æ¡£

## æ•…éšœå¤„ç†æµç¨‹

```mermaid
flowchart TD
    A[æ”¶åˆ°å‘Šè­¦] --> B[æ£€æŸ¥ Spark Job çŠ¶æ€]
    B --> C{Job çŠ¶æ€æ­£å¸¸?}
    C -->|æ˜¯| D[æ£€æŸ¥å…¶ä»–é—®é¢˜]
    C -->|å¦| E[æŸ¥çœ‹ Job è¯¦ç»†ä¿¡æ¯]
    
    E --> F{ç­‰å¾… Spark Master?}
    F -->|æ˜¯| G[å¢åŠ  Worker æ•°é‡]
    F -->|å¦| H[æ£€æŸ¥å…¶ä»–é”™è¯¯]
    
    G --> I[éªŒè¯ä¿®å¤æ•ˆæœ]
    I --> J{é—®é¢˜è§£å†³?}
    J -->|æ˜¯| K[è®°å½•è§£å†³æ–¹æ¡ˆ]
    J -->|å¦| L[è”ç³» @Junhan Ouyang]
    
    H --> M[æŸ¥çœ‹æ—¥å¿—åˆ†æ]
    M --> N[æ ¹æ®é”™è¯¯ç±»å‹å¤„ç†]
    N --> O[è®°å½•é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ]
```

## è”ç³»ä¿¡æ¯

### ç´§æ€¥æƒ…å†µ
- **ä¸»è¦è”ç³»äºº**: @Junhan Ouyang
- **è”ç³»æ–¹å¼**: é€šè¿‡ Slack æˆ–é‚®ä»¶è”ç³»
- **å“åº”æ—¶é—´**: ç´§æ€¥é—®é¢˜ 30 åˆ†é’Ÿå†…å“åº”

### éç´§æ€¥é—®é¢˜
- é€šè¿‡å·¥å•ç³»ç»Ÿæäº¤
- è¯¦ç»†æè¿°é—®é¢˜ç°è±¡å’Œå·²å°è¯•çš„è§£å†³æ–¹æ¡ˆ
- é™„ä¸Šç›¸å…³çš„æ—¥å¿—å’Œé…ç½®ä¿¡æ¯

---

**æ³¨æ„**: å¦‚æœé‡åˆ° "waiting for spark master ready" é—®é¢˜ï¼Œé¦–å…ˆå°è¯•å¢åŠ  tenant info worker æ•°é‡åˆ° 60 ä»¥ä¸Šï¼Œè¿™é€šå¸¸å¯ä»¥å¿«é€Ÿè§£å†³é—®é¢˜ã€‚å¦‚æœé—®é¢˜æŒç»­å­˜åœ¨ï¼Œè¯·åŠæ—¶è”ç³» @Junhan Ouyangã€‚






