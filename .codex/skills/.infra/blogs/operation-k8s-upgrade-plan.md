# Kubernetes é›†ç¾¤å‡çº§è®¡åˆ’æ–‡æ¡£ (v1.27 => v1.29)

### 1. master process (some notes)
```sh
---
# k get node -o wide | grep contro
ip-172-30-102-11.ec2.internal    Ready    control-plane   18m    v1.24.17   172.30.102.11    <none>           Ubuntu 22.04.5 LTS   6.8.0-1033-aws   containerd://1.7.27
ip-172-30-103-231.ec2.internal   Ready    control-plane   48m    v1.24.17   172.30.103.231   <none>           Ubuntu 22.04.5 LTS   6.8.0-1033-aws   containerd://1.7.27
ip-172-30-106-189.ec2.internal   Ready    control-plane   66m    v1.24.17   172.30.106.189   <none>           Ubuntu 22.04.5 LTS   6.8.0-1033-aws   containerd://1.7.27


ssh -i /Users/rshao/work/code_repos/infra_oncall_mgt/dv_kubeconfig/aws_pem/general-prod2022-useast.pem ubuntu@172.30.106.189

# master
watch -n 1 "kubectl get nodes | grep  control-plane"

# compoennts
watch -n 1 "kubectl get pod -n kube-system"
watch -n 1 'kubectl get pods -n kube-system | egrep "apiserver|controller|scheduler|etcd|coredns"'


# pending status
watch -n 1 "kubectl get pod -n kube-system | grep -v Running | grep -v Completed"


kubectl drain ip-172-30-106-189.ec2.internal --ignore-daemonsets


wait master node was ready before uncordon

kubectl uncordon ip-172-30-106-189.ec2.internal

wait 2mins...

finish all node and check all pods 

# [notice]:
check app 
- ä¸»è¦ä¿è¯ingress
    - jumpserver 
    - dapp
    - grafana
        - pod
        - sla
        - multi
    - datavisor internal ui (platform api server)
```

note:
```

ETCDCTL_API=3 etcdctl \
  --endpoints=https://172.30.106.189:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --key=/etc/kubernetes/pki/etcd/healthcheck-client.key \
  --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt \
  endpoint health --write-out=table



ETCDCTL_API=3 etcdctl \
  --endpoints=https://172.30.65.168:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --key=/etc/kubernetes/pki/etcd/healthcheck-client.key \
  --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt \
  member list --write-out=table

cordon 
drain 
etcd remove member
delete node 


```

### 2. components
```sh
root@ip-172-30-106-189:~/henry/1.24-1.26# ls */*
aws-cloud-controller-manager-chart-1.26/Chart.yaml   cluster-autoscaler-chart-1.26/README.md            ingress-nginx-chart/OWNERS               kube-state-metrics-chart-2.9/README.md
aws-cloud-controller-manager-chart-1.26/LICENSE      cluster-autoscaler-chart-1.26/README.md.gotmpl     ingress-nginx-chart/README.md            kube-state-metrics-chart-2.9/values.yaml
aws-cloud-controller-manager-chart-1.26/Readme.md    cluster-autoscaler-chart-1.26/valuesOverride.yaml  ingress-nginx-chart/README.md.gotmpl
aws-cloud-controller-manager-chart-1.26/values.yaml  cluster-autoscaler-chart-1.26/values.yaml          ingress-nginx-chart/values.yaml
cluster-autoscaler-chart-1.26/Chart.yaml             ingress-nginx-chart/Chart.yaml                     kube-state-metrics-chart-2.9/Chart.yaml

aws-cloud-controller-manager-chart-1.26/templates:
cluserrolebinding.yaml  clusterrole.yaml  daemonset.yaml  _helpers.tpl  NOTES.txt  rolebinding.yaml  serviceaccount.yaml

cluster-autoscaler-chart-1.26/templates:
clusterrolebinding.yaml  deployment.yaml  NOTES.txt  podsecuritypolicy.yaml            prometheusrule.yaml  role.yaml    serviceaccount.yaml  service.yaml
clusterrole.yaml         _helpers.tpl     pdb.yaml   priority-expander-configmap.yaml  rolebinding.yaml     secret.yaml  servicemonitor.yaml  vpa.yaml

ingress-nginx-chart/changelog:
helm-chart-2.10.0.md  helm-chart-3.0.0.md   helm-chart-3.18.0.md  helm-chart-3.30.0.md  helm-chart-3.8.0.md   helm-chart-4.0.6.md   helm-chart-4.1.2.md         helm-chart-4.8.1.md
helm-chart-2.11.0.md  helm-chart-3.10.0.md  helm-chart-3.19.0.md  helm-chart-3.3.0.md   helm-chart-3.9.0.md   helm-chart-4.0.7.md   helm-chart-4.2.0.md         helm-chart-4.8.2.md
helm-chart-2.11.1.md  helm-chart-3.10.1.md  helm-chart-3.20.0.md  helm-chart-3.31.0.md  helm-chart-4.0.10.md  helm-chart-4.0.9.md   helm-chart-4.2.1.md         helm-chart-4.8.3.md
helm-chart-2.11.2.md  helm-chart-3.11.0.md  helm-chart-3.20.1.md  helm-chart-3.3.1.md   helm-chart-4.0.11.md  helm-chart-4.10.0.md  helm-chart-4.3.0.md         helm-chart-4.9.0.md
helm-chart-2.11.3.md  helm-chart-3.11.1.md  helm-chart-3.21.0.md  helm-chart-3.32.0.md  helm-chart-4.0.12.md  helm-chart-4.10.1.md  helm-chart-4.4.0.md         helm-chart-4.9.1.md
helm-chart-2.12.0.md  helm-chart-3.12.0.md  helm-chart-3.22.0.md  helm-chart-3.33.0.md  helm-chart-4.0.13.md  helm-chart-4.10.2.md  helm-chart-4.5.2.md         helm-chart.md.gotmpl
helm-chart-2.12.1.md  helm-chart-3.13.0.md  helm-chart-3.23.0.md  helm-chart-3.34.0.md  helm-chart-4.0.14.md  helm-chart-4.1.0.md   helm-chart-4.6.0.md
helm-chart-2.13.0.md  helm-chart-3.14.0.md  helm-chart-3.24.0.md  helm-chart-3.4.0.md   helm-chart-4.0.15.md  helm-chart-4.11.0.md  helm-chart-4.6.1.md
helm-chart-2.14.0.md  helm-chart-3.15.0.md  helm-chart-3.25.0.md  helm-chart-3.5.0.md   helm-chart-4.0.18.md  helm-chart-4.11.1.md  helm-chart-4.7.0.md
helm-chart-2.15.0.md  helm-chart-3.15.1.md  helm-chart-3.26.0.md  helm-chart-3.5.1.md   helm-chart-4.0.1.md   helm-chart-4.11.2.md  helm-chart-4.7.1.md
helm-chart-2.16.0.md  helm-chart-3.16.0.md  helm-chart-3.27.0.md  helm-chart-3.6.0.md   helm-chart-4.0.2.md   helm-chart-4.11.3.md  helm-chart-4.7.2.md
helm-chart-2.9.0.md   helm-chart-3.16.1.md  helm-chart-3.28.0.md  helm-chart-3.7.0.md   helm-chart-4.0.3.md   helm-chart-4.11.4.md  helm-chart-4.8.0-beta.0.md
helm-chart-2.9.1.md   helm-chart-3.17.0.md  helm-chart-3.29.0.md  helm-chart-3.7.1.md   helm-chart-4.0.5.md   helm-chart-4.11.5.md  helm-chart-4.8.0.md

ingress-nginx-chart/ci:
admission-webhooks-cert-manager-values.yaml     controller-daemonset-opentelemetry-values.yaml   controller-deployment-podannotations-values.yaml  deamonset-psp-values.yaml
controller-configmap-addheaders-values.yaml     controller-daemonset-podannotations-values.yaml  controller-deployment-values.yaml                 deamonset-webhook-and-psp-values.yaml
controller-configmap-proxyheaders-values.yaml   controller-daemonset-values.yaml                 controller-hpa-values.yaml                        deployment-psp-values.yaml
controller-configmap-values.yaml                controller-deployment-extra-modules-values.yaml  controller-ingressclass-values.yaml               deployment-webhook-and-psp-values.yaml
controller-daemonset-extra-modules-values.yaml  controller-deployment-metrics-values.yaml        controller-service-internal-values.yaml
controller-daemonset-metrics-values.yaml        controller-deployment-opentelemetry-values.yaml  controller-service-values.yaml

ingress-nginx-chart/templates:
admission-webhooks                      controller-daemonset.yaml             controller-prometheusrule.yaml    controller-servicemonitor.yaml            default-backend-psp.yaml
clusterrolebinding.yaml                 controller-deployment.yaml            controller-psp.yaml               controller-service-webhook.yaml           default-backend-rolebinding.yaml
clusterrole.yaml                        controller-hpa.yaml                   controller-rolebinding.yaml       controller-service.yaml                   default-backend-role.yaml
controller-configmap-addheaders.yaml    controller-ingressclass-aliases.yaml  controller-role.yaml              default-backend-deployment.yaml           default-backend-serviceaccount.yaml
controller-configmap-proxyheaders.yaml  controller-ingressclass.yaml          controller-secret.yaml            default-backend-extra-configmaps.yaml     default-backend-service.yaml
controller-configmap-tcp.yaml           controller-keda.yaml                  controller-serviceaccount.yaml    default-backend-hpa.yaml                  _helpers.tpl
controller-configmap-udp.yaml           controller-networkpolicy.yaml         controller-service-internal.yaml  default-backend-networkpolicy.yaml        NOTES.txt
controller-configmap.yaml               controller-poddisruptionbudget.yaml   controller-service-metrics.yaml   default-backend-poddisruptionbudget.yaml  _params.tpl

ingress-nginx-chart/tests:
admission-webhooks                           controller-hpa_test.yaml                   controller-prometheusrule_test.yaml    controller-service-webhook_test.yaml
controller-configmap-addheaders_test.yaml    controller-ingressclass-aliases_test.yaml  controller-serviceaccount_test.yaml    default-backend-deployment_test.yaml
controller-configmap-proxyheaders_test.yaml  controller-ingressclass_test.yaml          controller-service-internal_test.yaml  default-backend-extra-configmaps_test.yaml
controller-configmap_test.yaml               controller-keda_test.yaml                  controller-service-metrics_test.yaml   default-backend-poddisruptionbudget_test.yaml
controller-daemonset_test.yaml               controller-networkpolicy_test.yaml         controller-servicemonitor_test.yaml    default-backend-serviceaccount_test.yaml
controller-deployment_test.yaml              controller-poddisruptionbudget_test.yaml   controller-service_test.yaml           default-backend-service_test.yaml

kube-state-metrics-chart-2.9/templates:
ciliumnetworkpolicy.yaml  extra-manifests.yaml    NOTES.txt                    psp-clusterrole.yaml  serviceaccount.yaml            stsdiscovery-role.yaml
clusterrolebinding.yaml   _helpers.tpl            pdb.yaml                     rbac-configmap.yaml   servicemonitor.yaml            verticalpodautoscaler.yaml
crs-configmap.yaml        kubeconfig-secret.yaml  podsecuritypolicy.yaml       rolebinding.yaml      service.yaml
deployment.yaml           networkpolicy.yaml      psp-clusterrolebinding.yaml  role.yaml             stsdiscovery-rolebinding.yaml
root@ip-172-30-106-189:~/henry/1.24-1.26# 
```
```
helm 
```
## å‡çº§æµç¨‹æ¦‚è§ˆ

### æ ¸å¿ƒå‡çº§æµç¨‹

**å‡çº§ç­–ç•¥**ï¼šåˆ†é˜¶æ®µæ¸è¿›å¼ï¼Œv1.27 â†’ v1.28 â†’ v1.29ï¼ˆå¿…é¡»ç»è¿‡ä¸­é—´ç‰ˆæœ¬ï¼‰

```mermaid
graph TD
    A[é˜¶æ®µ0: å‡çº§å‰å‡†å¤‡] --> B[é˜¶æ®µ1: Master èŠ‚ç‚¹å‡çº§ 1.27â†’1.28]
    B --> C[é˜¶æ®µ2: Master èŠ‚ç‚¹å‡çº§ 1.28â†’1.29]
    C --> D[é˜¶æ®µ3: ç»„ä»¶å‡çº§]
    D --> E[é˜¶æ®µ4: Worker èŠ‚ç‚¹å‡çº§ 1.27â†’1.29]
    E --> F[é˜¶æ®µ5: å…¨é¢éªŒè¯]
    F --> G[å‡çº§å®Œæˆ]
    
    subgraph "é˜¶æ®µ0: å‡çº§å‰å‡†å¤‡"
        A1[é›†ç¾¤ä¿¡æ¯æ”¶é›†] --> A2[etcd å¤‡ä»½]
        A2 --> A3[å…³é”®æœåŠ¡è¯†åˆ«]
        A3 --> A4[å‡çº§è®¡åˆ’éªŒè¯]
    end
    
    subgraph "é˜¶æ®µ1-2: Master å‡çº§"
        B1[ç¬¬ä¸€ä¸ª Master] --> B2[ç¬¬äºŒä¸ª Master]
        B2 --> B3[ç¬¬ä¸‰ä¸ª Master]
        C1[ç¬¬ä¸€ä¸ª Master] --> C2[ç¬¬äºŒä¸ª Master]
        C2 --> C3[ç¬¬ä¸‰ä¸ª Master]
    end
    
    subgraph "é˜¶æ®µ3: ç»„ä»¶å‡çº§"
        D1[Calico CNI] --> D2[AWS Cloud Provider]
        D2 --> D3[Cluster Autoscaler]
        D3 --> D4[Kube State Metrics]
        D4 --> D5[Ingress Nginx]
    end
    
    subgraph "é˜¶æ®µ4: Worker å‡çº§"
        E1[æ•°æ®åº“èŠ‚ç‚¹ä¼˜å…ˆ] --> E2[ä¸šåŠ¡æœåŠ¡èŠ‚ç‚¹]
        E2 --> E3[å…¶ä»–å·¥ä½œèŠ‚ç‚¹]
    end
    
    subgraph "é˜¶æ®µ5: éªŒè¯"
        F1[Pod çŠ¶æ€æ£€æŸ¥] --> F2[æœåŠ¡é€šä¿¡éªŒè¯]
        F2 --> F3[Ingress è®¿é—®éªŒè¯]
        F3 --> F4[æ•°æ®åº“æœåŠ¡éªŒè¯]
        F4 --> F5[ç›‘æ§å‘Šè­¦æ£€æŸ¥]
    end
    
    style A fill:#e1f5fe
    style G fill:#c8e6c9
    style E1 fill:#fff3e0
    style A2 fill:#ffebee
```

### å‡çº§æ­¥éª¤è¯¦è§£

#### é˜¶æ®µ0: å‡çº§å‰å‡†å¤‡
- é›†ç¾¤ä¿¡æ¯æ”¶é›†ï¼šèŠ‚ç‚¹åˆ—è¡¨ã€æ•°æ®åº“/ä¸šåŠ¡æœåŠ¡èŠ‚ç‚¹è¯†åˆ«ã€ASG/Launch Template æ£€æŸ¥
- æ•°æ®å¤‡ä»½ï¼šetcdï¼ˆå¿…é¡»ï¼‰ã€æ•°æ®åº“ã€å…³é”®åº”ç”¨é…ç½®
- å‡çº§è®¡åˆ’éªŒè¯ï¼škubeadm å‡çº§è®¡åˆ’ã€ç»„ä»¶å…¼å®¹æ€§ã€å›æ»šæ–¹æ¡ˆ

#### é˜¶æ®µ1-2: Master èŠ‚ç‚¹å‡çº§
- **1.27 â†’ 1.28**ï¼šé€ä¸ªå‡çº§ Masterï¼ˆä¿æŒè‡³å°‘ 2 ä¸ªå¯ç”¨ï¼‰ï¼Œdrain â†’ kubeadm â†’ kubelet â†’ uncordon
- **1.28 â†’ 1.29**ï¼šé‡å¤ä¸Šè¿°æµç¨‹ï¼Œç¡®ä¿ç‰ˆæœ¬ä¸€è‡´

#### é˜¶æ®µ3: ç»„ä»¶å‡çº§
æŒ‰é¡ºåºï¼šCalico CNI â†’ AWS Cloud Provider â†’ Cluster Autoscaler â†’ Kube State Metrics â†’ Ingress Nginx

#### é˜¶æ®µ4: Worker èŠ‚ç‚¹å‡çº§
- æ•°æ®åº“èŠ‚ç‚¹ä¼˜å…ˆ â†’ ä¸šåŠ¡æœåŠ¡èŠ‚ç‚¹ â†’ å…¶ä»–å·¥ä½œèŠ‚ç‚¹
- é€ä¸ªå‡çº§ï¼ŒéªŒè¯æœåŠ¡å¥åº·

#### é˜¶æ®µ5: å…¨é¢éªŒè¯
Pod çŠ¶æ€ã€æœåŠ¡é€šä¿¡ã€Ingress è®¿é—®ã€æ•°æ®åº“æœåŠ¡ã€ç›‘æ§å‘Šè­¦

### å…³é”®ä¾èµ–å…³ç³»

```
Master å‡çº§ â†’ ç»„ä»¶å‡çº§ â†’ Worker å‡çº§
```

## ç”Ÿäº§ç¯å¢ƒå…³é”®æ³¨æ„äº‹é¡¹

### âš ï¸ é«˜é£é™©æ“ä½œè­¦å‘Š

#### 1. Master èŠ‚ç‚¹å‡çº§é£é™©

**é£é™©**ï¼šAPI Server ä¸­æ–­ã€etcd æ•°æ®æŸåã€ç‰ˆæœ¬ä¸ä¸€è‡´

**é¢„é˜²**ï¼š
- å¤‡ä»½ etcdï¼ˆå¿…é¡»ï¼‰
- é€ä¸ªå‡çº§ Masterï¼ˆä¿æŒè‡³å°‘ 2 ä¸ªå¯ç”¨ï¼‰
- éªŒè¯å‡çº§è®¡åˆ’ï¼š`kubeadm upgrade plan`
- ç›‘æ§ API Server å¥åº·çŠ¶æ€
- å‡†å¤‡å›æ»šæ–¹æ¡ˆ

**é—®é¢˜æ’æŸ¥**ï¼š`kubeadm upgrade apply` å¤±è´¥ â†’ æ£€æŸ¥ etcd å¥åº·ï¼›æ— æ³•åŠ å…¥é›†ç¾¤ â†’ æ£€æŸ¥ç½‘ç»œ/è¯ä¹¦ï¼›API Server æ— æ³•å¯åŠ¨ â†’ æ£€æŸ¥ kubelet æ—¥å¿—

#### 2. Worker èŠ‚ç‚¹å‡çº§é£é™©

**é£é™©**ï¼šPod è¿ç§»å¤±è´¥ã€æ•°æ®åº“/ä¸šåŠ¡æœåŠ¡ä¸­æ–­ã€ASG è‡ªåŠ¨æ‰©å±•å¯¼è‡´ç‰ˆæœ¬ä¸ä¸€è‡´

**é¢„é˜²**ï¼š
- æ•°æ®åº“èŠ‚ç‚¹ä¼˜å…ˆå‡çº§
- é€ä¸ªå‡çº§èŠ‚ç‚¹ï¼Œç­‰å¾…å°±ç»ªåå†ä¸‹ä¸€ä¸ª
- æ£€æŸ¥ Pod åˆ†å¸ƒï¼Œç¡®ä¿æœ‰è¶³å¤ŸèŠ‚ç‚¹
- ä¸´æ—¶è°ƒæ•´ ASG MinSize/MaxSizeï¼Œé˜²æ­¢è‡ªåŠ¨æ‰©å±•
- drain ä½¿ç”¨ `--timeout=300s`
- ä½¿ç”¨ `kubectl wait` éªŒè¯èŠ‚ç‚¹å°±ç»ª

**é—®é¢˜æ’æŸ¥**ï¼šdrain å¤±è´¥ â†’ æ£€æŸ¥ PDB/DaemonSetï¼›Pod æ— æ³•è¿ç§» â†’ æ£€æŸ¥èµ„æº/è°ƒåº¦ç­–ç•¥ï¼›æ— æ³•åŠ å…¥é›†ç¾¤ â†’ æ£€æŸ¥ kubelet/ç½‘ç»œï¼›ASG åˆ›å»ºæ—§èŠ‚ç‚¹ â†’ æ›´æ–° Launch Template

#### 3. ç»„ä»¶å‡çº§é£é™©

**é£é™©**ï¼šCalico ç½‘ç»œä¸­æ–­ã€LoadBalancer ä¸­æ–­ã€Autoscaler å¼‚å¸¸ã€Ingress è®¿é—®ä¸­æ–­

**é¢„é˜²**ï¼š
- ä¸¥æ ¼æŒ‰é¡ºåºï¼šCalico â†’ Cloud Provider â†’ Autoscaler â†’ Metrics â†’ Ingress
- å‡çº§åç«‹å³éªŒè¯ï¼šç½‘ç»œè¿é€šæ€§ã€LoadBalancerã€Ingress è®¿é—®
- æ£€æŸ¥ç»„ä»¶æ—¥å¿—

**é—®é¢˜æ’æŸ¥**ï¼šCalico Pod æ— æ³•é€šä¿¡ â†’ æ£€æŸ¥ Pod çŠ¶æ€/ç½‘ç»œç­–ç•¥ï¼›LoadBalancer æ— æ³•åˆ›å»º â†’ æ£€æŸ¥ IAM/é…ç½®ï¼›Ingress å¤±è´¥ â†’ æ£€æŸ¥ Controller/Serviceï¼›Autoscaler å¼‚å¸¸ â†’ æ£€æŸ¥ ASG/IAM

#### 4. AWS ç›¸å…³é£é™©

**é£é™©**ï¼šLaunch Template ç‰ˆæœ¬ã€å®ä¾‹ç±»å‹å…¼å®¹æ€§ã€LoadBalancer é‡æ–°åˆ›å»ºã€IAM æƒé™ä¸è¶³

**é¢„é˜²**ï¼š
- å‡çº§å‰æ›´æ–°æ‰€æœ‰ ASG Launch Template
- éªŒè¯å®ä¾‹ç±»å‹å…¼å®¹æ€§
- æ£€æŸ¥å¹¶æ›´æ–° IAM æƒé™
- å¤‡ä»½ LoadBalancer é…ç½®
- ç›‘æ§ EC2/ELB/ASG çŠ¶æ€

**é—®é¢˜æ’æŸ¥**ï¼šASG åˆ›å»ºæ—§èŠ‚ç‚¹ â†’ æ›´æ–° Launch Template ä¸º `$Latest`ï¼›LoadBalancer æ— æ³•åˆ›å»º â†’ æ£€æŸ¥ IAM/å®‰å…¨ç»„ï¼›å®ä¾‹æ— æ³•åŠ å…¥ â†’ æ£€æŸ¥ç”¨æˆ·æ•°æ®/kubeletï¼›ç½‘ç»œé—®é¢˜ â†’ æ£€æŸ¥ VPC/å®‰å…¨ç»„

#### 5. æ•°æ®åº“æœåŠ¡é£é™©

**é£é™©**ï¼šæœåŠ¡ä¸­æ–­ã€æ•°æ®ä¸€è‡´æ€§ã€è¿æ¥ä¸­æ–­

**é¢„é˜²**ï¼š
- æ•°æ®åº“èŠ‚ç‚¹ä¼˜å…ˆå‡çº§
- æ¯ä¸ªèŠ‚ç‚¹å‡çº§åç«‹å³éªŒè¯æœåŠ¡å¥åº·
- éªŒè¯åº”ç”¨ä¸æ•°æ®åº“è¿æ¥
- ç›‘æ§æ•°æ®åº“æ€§èƒ½æŒ‡æ ‡

**é—®é¢˜æ’æŸ¥**ï¼šMySQL ä¸­æ–­ â†’ æ£€æŸ¥ Pod/æŒä¹…åŒ–å­˜å‚¨ï¼›Kafka ä¸å¯ç”¨ â†’ æ£€æŸ¥ Pod/Zookeeperï¼›YugabyteDB åˆ†è£‚ â†’ æ£€æŸ¥ YB-Master/TServerï¼›ClickHouse å¤±è´¥ â†’ æ£€æŸ¥ Pod/æ•°æ®ç›®å½•

#### 6. ä¸šåŠ¡æœåŠ¡é£é™©

**é£é™©**ï¼šæœåŠ¡ä¸­æ–­ã€dcluster å—å½±å“ã€Ingress è®¿é—®ä¸­æ–­

**é¢„é˜²**ï¼š
- ä¸šåŠ¡ä½å³°æœŸå‡çº§
- å‡çº§åç«‹å³éªŒè¯å…³é”®æœåŠ¡
- éªŒè¯æ‰€æœ‰ Ingress åŸŸåè®¿é—®
- ç›‘æ§ä¸šåŠ¡å…³é”®æŒ‡æ ‡

**é—®é¢˜æ’æŸ¥**ï¼šPod æ— æ³•å¯åŠ¨ â†’ æ£€æŸ¥èµ„æº/é•œåƒï¼›dcluster ä¸å¯ç”¨ â†’ æ£€æŸ¥ Pod/é…ç½®ï¼›Ingress 502/503 â†’ æ£€æŸ¥ Controller/åç«¯ï¼›å“åº”æ—¶é—´å¢åŠ  â†’ æ£€æŸ¥èµ„æº/ç½‘ç»œ

### ğŸ” å…³é”®æ£€æŸ¥ç‚¹

**Master èŠ‚ç‚¹å‡çº§å**ï¼šAPI Server å¥åº·ã€etcd çŠ¶æ€ã€ç‰ˆæœ¬ä¸€è‡´æ€§ã€ç³»ç»Ÿ Pod çŠ¶æ€

**Worker èŠ‚ç‚¹å‡çº§å**ï¼šèŠ‚ç‚¹ Readyã€Pod çŠ¶æ€ã€ç½‘ç»œè¿é€šæ€§ã€èµ„æºä½¿ç”¨

**ç»„ä»¶å‡çº§å**ï¼šç»„ä»¶ Pod çŠ¶æ€ã€åŠŸèƒ½éªŒè¯ã€æ—¥å¿—æ£€æŸ¥ã€æœåŠ¡è®¿é—®

**å‡çº§å®Œæˆå**ï¼šèŠ‚ç‚¹ç‰ˆæœ¬ä¸€è‡´ã€Pod Runningã€æœåŠ¡æ­£å¸¸ã€ç›‘æ§æ­£å¸¸ã€æ•°æ®åº“æ­£å¸¸ã€Ingress æ­£å¸¸

### ğŸ“‹ å›æ»šå‡†å¤‡

**è§¦å‘æ¡ä»¶**ï¼šMaster å‡çº§å¤±è´¥ã€>50% Pod é Runningã€å…³é”®æœåŠ¡æ— æ³•è®¿é—®ã€å¤§é‡å‘Šè­¦

**å›æ»šæ­¥éª¤**ï¼šåœæ­¢å‡çº§ â†’ å›æ»š Master â†’ å›æ»š Worker â†’ å›æ»šç»„ä»¶ â†’ æ¢å¤ etcd â†’ éªŒè¯é›†ç¾¤

### â° æ—¶é—´çª—å£

- **æµ‹è¯•ç¯å¢ƒ**ï¼šå·¥ä½œæ—¥ç™½å¤©
- **é¢„ç”Ÿäº§ç¯å¢ƒ**ï¼šå·¥ä½œæ—¥æ™šä¸Š
- **ç”Ÿäº§ç¯å¢ƒ**ï¼šå‘¨æœ«/èŠ‚å‡æ—¥
- **é¢„è®¡æ—¶é—´**ï¼šå°å‹ <10 èŠ‚ç‚¹ï¼ˆ2-4hï¼‰ã€ä¸­å‹ 10-50 èŠ‚ç‚¹ï¼ˆ4-8hï¼‰ã€å¤§å‹ >50 èŠ‚ç‚¹ï¼ˆ8-16hï¼‰

## é›†ç¾¤åŸºæœ¬ä¿¡æ¯

```yaml
é›†ç¾¤åç§°: aws-useast1-dev-c
åŒºåŸŸ: us-east-1
IMDSç‰ˆæœ¬: IMDSv2
AWS ASG: [å¾…è¡¥å……]
```
```sh
aws autoscaling describe-auto-scaling-groups --region us-east-1 | grep -A5 -B5 "aws-useast1-dev-c" | grep AutoScalingGroupName

        "AutoScalingGroupName": "aws-useast1-dev-c-preprod-dedicated-m7a2xlarge",
        "AutoScalingGroupName": "aws-useast1-dev-c-preprod-dedicated-r6i.2xlarge",
        "AutoScalingGroupName": "aws-useast1-dev-c-private-r6i.2xlarge",
        "AutoScalingGroupName": "aws-useast1-dev-c-r6i.2xlarge",
        "AutoScalingGroupName": "aws-useast1-dev-c-r6i.2xlarge-preprod",
        "AutoScalingGroupName": "aws-useast1-dev-c-r6i.4xlarge",
```

### east-mgt 
```sh
â¯ keastmgt get nodes | grep -vE 'control-plane'
keastmgt get nodes | grep -E 'control-plane'
NAME                             STATUS   ROLES           AGE    VERSION
ip-10-151-130-63.ec2.internal    Ready    <none>          342d   v1.27.6
ip-10-151-131-131.ec2.internal   Ready    <none>          107d   v1.27.6
ip-10-151-136-102.ec2.internal   Ready    <none>          216d   v1.27.6
ip-10-151-136-189.ec2.internal   Ready    <none>          177d   v1.27.6

ip-10-151-134-132.ec2.internal   Ready    control-plane   53d    v1.27.6
ip-10-151-136-220.ec2.internal   Ready    control-plane   342d   v1.27.6
ip-10-151-140-52.ec2.internal    Ready    control-plane   342d   v1.27.6


â¯ aws autoscaling describe-auto-scaling-groups \
  --region us-east-1 \
  --query 'AutoScalingGroups[?contains(AutoScalingGroupName, `mgt`)].{ASG:AutoScalingGroupName, LT:LaunchTemplate, LC:LaunchConfigurationName}' \
  --output table | cat
-------------------------------------------------------------------------
|                       DescribeAutoScalingGroups                       |
+---------------------------------------------------------+-------------+
|                           ASG                           |     LC      |
+---------------------------------------------------------+-------------+
|  aws-useast1-mgt-a-r6i.2xlarge                          |  None       |
+---------------------------------------------------------+-------------+
||                                 LT                                  ||
|+-----------------------+---------------------------------+-----------+|
||   LaunchTemplateId    |       LaunchTemplateName        |  Version  ||
|+-----------------------+---------------------------------+-----------+|
||  lt-0cd88ce4dad474688 |  aws-useast1-mgt-a-r6i.2xlarge  |  $Latest  ||
|+-----------------------+---------------------------------+-----------+|
|                       DescribeAutoScalingGroups                       |
+---------------------------------------------------------+-------------+
|                           ASG                           |     LC      |
+---------------------------------------------------------+-------------+
|  aws-useast1-mgt-a-r6i.4xlarge                          |  None       |
+---------------------------------------------------------+-------------+
||                                 LT                                  ||
|+-----------------------+---------------------------------+-----------+|
||   LaunchTemplateId    |       LaunchTemplateName        |  Version  ||
|+-----------------------+---------------------------------+-----------+|
||  lt-0c78043668e38e11c |  aws-useast1-mgt-a-r6i.4xlarge  |  $Default ||
|+-----------------------+---------------------------------+-----------+|

â¯ aws ec2 describe-instances \
  --region us-east-1 \
  --instance-ids i-0ae670b580e743177 i-0542600a581746175 i-0937a59b756a6bf8e i-04f011ea61fbb6807 \
  --query 'Reservations[].Instances[].{ID:InstanceId, AMI:ImageId}' \
  --output table | cat

--------------------------------------------------
|                DescribeInstances               |
+------------------------+-----------------------+
|           AMI          |          ID           |
+------------------------+-----------------------+
|  ami-0cf2efaff9c48a3de |  i-0ae670b580e743177  |
|  ami-0cf2efaff9c48a3de |  i-0542600a581746175  |
|  ami-0cf2efaff9c48a3de |  i-0937a59b756a6bf8e  |
|  ami-0c6e923a1ac84deeb |  i-04f011ea61fbb6807  |
+------------------------+-----------------------+

â¯ aws ec2 describe-instances \
  --region us-east-1 \
  --instance-ids $(aws autoscaling describe-auto-scaling-groups \
    --region us-east-1 \
    --query 'AutoScalingGroups[?contains(AutoScalingGroupName, `mgt`)].Instances[].InstanceId' \
    --output text) \
  --query 'Reservations[].Instances[].{ID:InstanceId, PrivateIP:PrivateIpAddress, PublicIP:PublicIpAddress}' \
  --output table| cat

-------------------------------------------------------
|                  DescribeInstances                  |
+----------------------+------------------+-----------+
|          ID          |    PrivateIP     | PublicIP  |
+----------------------+------------------+-----------+
|  i-0ae670b580e743177 |  10.151.130.63   |  None     |
|  i-0542600a581746175 |  10.151.136.102  |  None     |
|  i-0937a59b756a6bf8e |  10.151.136.189  |  None     |
|  i-04f011ea61fbb6807 |  10.151.131.131  |  None     |
+----------------------+------------------+-----------+
```

## å‡çº§æ¦‚è§ˆ

æœ¬æ–‡æ¡£æè¿°ä» Kubernetes v1.27 å‡çº§åˆ° v1.29 çš„è¯¦ç»†æ­¥éª¤å’Œæ£€æŸ¥æ¸…å•ã€‚

```mermaid
graph TD
    A[å¼€å§‹: é›†ç¾¤ä¿¡æ¯æ”¶é›†] --> B[Master èŠ‚ç‚¹å‡çº§]
    B --> C[Worker èŠ‚ç‚¹å‡çº§]
    C --> D[ç»„ä»¶å‡çº§]
    D --> E[éªŒè¯æ£€æŸ¥]
    E --> F[å‡çº§å®Œæˆ]
    
    subgraph "Worker èŠ‚ç‚¹å‡çº§"
        C1[æ•°æ®åº“èŠ‚ç‚¹] --> C2[å…¶ä»–å·¥ä½œèŠ‚ç‚¹]
    end
    
    subgraph "ç»„ä»¶å‡çº§"
        D1[Cloud Provider] --> D2[Cluster Autoscaler]
        D2 --> D3[Kube State Metrics]
        D3 --> D4[Ingress Nginx Controller]
    end
    
    subgraph "éªŒè¯æ£€æŸ¥"
        E1[Pod çŠ¶æ€æ£€æŸ¥] --> E2[é€šä¿¡éªŒè¯]
        E2 --> E3[Ingress è®¿é—®éªŒè¯]
    end
    
    style A fill:#e1f5fe
    style F fill:#c8e6c9
```

## è¯¦ç»†å‡çº§æ­¥éª¤

### æ­¥éª¤1: é›†ç¾¤ä¿¡æ¯æ”¶é›†

#### 1.1 åŸºæœ¬ä¿¡æ¯æ”¶é›†

```bash
# æ£€æŸ¥é›†ç¾¤ç‰ˆæœ¬
kubectl version --short

# æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€
kubectl get nodes -o wide

# æ£€æŸ¥é›†ç¾¤å¥åº·çŠ¶æ€
kubectl get componentstatuses

# æŸ¥çœ‹ AWS ASG ä¿¡æ¯
aws autoscaling describe-auto-scaling-groups --region us-east-1 | grep -A5 -B5 "aws-useast1-dev-c"
```

#### 1.2 dcluster ç¯å¢ƒä¿¡æ¯

```bash
# æŸ¥çœ‹ dcluster å‘½åç©ºé—´
kubectl get pod -A | grep -i dcluster | awk '{print $1}' | sort | uniq -c | awk '{print $2}'

# å·²çŸ¥å‘½åç©ºé—´:
# - qa-oneclick

# æ£€æŸ¥ dcluster pod åˆ†å¸ƒ
kubectl get pods -A | grep dcluster | head -10
```

#### 1.3 å‡çº§å‰é›†ç¾¤çŠ¶æ€ä¿å­˜

åœ¨å¼€å§‹å‡çº§å‰ï¼Œå¿…é¡»ä¿å­˜å½“å‰é›†ç¾¤çŠ¶æ€ï¼Œä¾¿äºå¯¹æ¯”å’Œå›æ»šï¼š

```bash
# 1. ä¿å­˜ Pod çŠ¶æ€
kubectl get pods --all-namespaces -o wide > cluster_pods_before_upgrade.txt

# 2. æ£€æŸ¥ Endpoint çŠ¶æ€ï¼ˆé‡ç‚¹å…³æ³¨ None çŠ¶æ€ï¼‰
kubectl get endpoints --all-namespaces | grep -i none > endpoints_issues_before_upgrade.txt

# 3. ä¿å­˜èŠ‚ç‚¹çŠ¶æ€
kubectl get nodes -o wide > cluster_nodes_before_upgrade.txt

# 4. ä¿å­˜å…³é”®æœåŠ¡çŠ¶æ€
kubectl get svc --all-namespaces > cluster_services_before_upgrade.txt

# 5. ä¿å­˜ Ingress çŠ¶æ€
kubectl get ingress --all-namespaces > cluster_ingress_before_upgrade.txt
```

**æ³¨æ„**ï¼šå¦‚æœ endpoint æ˜¾ç¤º `none`ï¼Œé€šå¸¸è¡¨ç¤ºæœåŠ¡å¼‚å¸¸ï¼Œéœ€è¦é‡å¯ Pod æ‰èƒ½æ¢å¤ã€‚

#### 1.4 Kafka MirrorMaker ä¸æµé‡ç®¡ç†

åœ¨åŒé›†ç¾¤ç¯å¢ƒï¼ˆCluster A å’Œ Cluster Bï¼‰ä¸­å‡çº§æ—¶ï¼Œéœ€è¦ç‰¹åˆ«æ³¨æ„ Kafka MirrorMaker çš„ç®¡ç†å’Œæµé‡åˆ‡æ¢ç­–ç•¥ã€‚

##### MirrorMaker å·¥ä½œåŸç†

MirrorMaker è´Ÿè´£åœ¨ä¸¤ä¸ªé›†ç¾¤ä¹‹é—´åŒæ­¥ Kafka æ•°æ®ï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§å’Œé«˜å¯ç”¨æ€§ã€‚

**Grafana ç›‘æ§é¢æ¿**ï¼š
- [MirrorLag ç›‘æ§](https://grafana-mgt.dv-api.com/d/-N7cUPZNk/mirrorlag-v2?orgId=1&var-cluster=aws-uswest2-prod-b&var-namespace=prod&var-source=cluster_b&var-target=cluster_a&var-topic=All)

##### å‡çº§ Cluster A çš„æµé‡ç®¡ç†ç­–ç•¥

```mermaid
graph TD
    A[åˆå§‹çŠ¶æ€: æµé‡åœ¨A] --> B[åˆ‡æµå‡†å¤‡]
    B --> C[æµé‡åˆ‡åˆ°B]
    C --> D[ç­‰å¾…æµé‡ç¨³å®š]
    D --> E[å…³é—­MirrorMaker]
    E --> F[å¼€å§‹å‡çº§A]
    F --> G[å‡çº§å®Œæˆ]
    G --> H[å¯åŠ¨Bâ†’A MirrorMaker]
    H --> I[ç­‰å¾…æ•°æ®åŒæ­¥]
    I --> J[éªŒè¯Aé›†ç¾¤]
    J --> K[æµé‡åˆ‡å›A]
    
    style A fill:#e1f5fe
    style C fill:#fff3e0
    style E fill:#ffebee
    style F fill:#f3e5f5
    style H fill:#e8f5e9
    style K fill:#c8e6c9
```

**è¯¦ç»†æ­¥éª¤**ï¼š

**é˜¶æ®µ1ï¼šæµé‡åˆ‡æ¢å‰ï¼ˆå‡çº§ Cluster Aï¼‰**
```bash
# å½“å‰çŠ¶æ€ï¼š
# - æµé‡åœ¨ Cluster A
# - MirrorMaker æ­£å¸¸å·¥ä½œï¼ˆA â†” B åŒå‘åŒæ­¥ï¼‰
# - æ— æ˜æ˜¾ Lag

# ç›‘æ§å½“å‰çŠ¶æ€
# æŸ¥çœ‹ MirrorMaker lag æƒ…å†µï¼ˆåº”è¯¥å¾ˆå¹³ç¨³ï¼‰
```

**é˜¶æ®µ2ï¼šæµé‡åˆ‡æ¢åˆ° Cluster B**
```bash
# 1. åˆ‡æµåˆ° Cluster B
# ï¼ˆé€šè¿‡ API Gateway/Load Balancer ç­‰æµé‡æ§åˆ¶æœºåˆ¶ï¼‰

# 2. ç­‰å¾…æµé‡å®Œå…¨åˆ‡æ¢åˆ° Bï¼ˆè§‚å¯Ÿ Grafana é¢æ¿ï¼‰
# - æ­¤æ—¶æµé‡åœ¨ B
# - MirrorMaker ä¼šå°† B çš„æ•°æ®åŒæ­¥åˆ° A
# - B â†’ A çš„ lag ä¼šä¿æŒå¹³ç¨³

# 3. å…³é—­åŒå‘ MirrorMaker
# å…³é—­ A â†’ B çš„ MirrorMaker
kubectl scale deployment mirrormaker-a-to-b --replicas=0 -n <namespace>

# å…³é—­ B â†’ A çš„ MirrorMakerï¼ˆé‡ç‚¹ï¼ï¼‰
kubectl scale deployment mirrormaker-b-to-a --replicas=0 -n <namespace>

# é¢„æœŸç»“æœï¼š
# - B â†’ A çš„ lag ä¼šå¼€å§‹ä¸Šå‡ï¼ˆå› ä¸ºé˜»æ­¢äº†æ•°æ®æµå…¥ Aï¼‰
# - è¿™æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸ºæˆ‘ä»¬è¦å¯¹ A è¿›è¡Œç»´æŠ¤æ“ä½œ
```

**é˜¶æ®µ3ï¼šå‡çº§ Cluster A**
```bash
# æ­¤æ—¶ï¼š
# - æµé‡åœ¨ Bï¼ˆä¸šåŠ¡æ­£å¸¸è¿è¡Œï¼‰
# - MirrorMaker å·²å…³é—­
# - A é›†ç¾¤å¯ä»¥å®‰å…¨å‡çº§

# å¼€å§‹å‡çº§ Cluster Aï¼ˆå‚è€ƒåç»­å‡çº§æ­¥éª¤ï¼‰
```

**MirrorMaker Lag å…³é”®æŒ‡æ ‡è¯´æ˜**ï¼š

| åœºæ™¯ | æµé‡ä½ç½® | MirrorMaker çŠ¶æ€ | Aâ†’B Lag | Bâ†’A Lag | è¯´æ˜ |
|------|---------|-----------------|---------|---------|------|
| åˆå§‹çŠ¶æ€ | A | Aâ†”B åŒå‘è¿è¡Œ | ä½ | ä½ | æ­£å¸¸è¿è¡Œ |
| åˆ‡æµå | B | Aâ†”B åŒå‘è¿è¡Œ | ä¸Šå‡ | å¹³ç¨³ | B åœ¨æ¥æ”¶æµé‡ï¼Œå‘ A åŒæ­¥ |
| å…³é—­ MirrorMaker | B | å·²å…³é—­ | - | ä¸Šå‡ | Bâ†’A é˜»å¡ï¼Œlag ä¸Šå‡æ­£å¸¸ |
| å‡çº§ä¸­ | B | å·²å…³é—­ | - | æŒç»­ä¸Šå‡ | A å‡çº§ä¸­ï¼Œæ— æ³•æ¥æ”¶æ•°æ® |
| å¯åŠ¨ MirrorMaker | B | Bâ†’A è¿è¡Œ | - | ä¸‹é™ | B å¼€å§‹å‘ A åŒæ­¥æ•°æ® |
| å®ŒæˆåŒæ­¥ | B | Bâ†’A è¿è¡Œ | - | ä½ | æ•°æ®åŒæ­¥å®Œæˆ |
| åˆ‡æµå› A | A | Aâ†”B åŒå‘è¿è¡Œ | å¹³ç¨³ | ä½ | æ¢å¤æ­£å¸¸è¿è¡Œ |

#### 1.5: Ingress Nginx å®‰è£…ä¸éªŒè¯

##### 1.5.1 å‡†å¤‡ Helm Chart

ç¡®ä¿ä»¥ä¸‹ç›®å½•ç»“æ„å­˜åœ¨ï¼š

```plaintext
ingress-nginx/
â””â”€â”€ ingress-nginx-chart/
    â”œâ”€â”€ Chart.yaml
    â”œâ”€â”€ values.yaml
    â””â”€â”€ templates/
```

##### 1.5.2 éƒ¨ç½² Ingress Controller

ä½¿ç”¨ Helm å®‰è£…æˆ–å‡çº§ ingress-nginxï¼š

```bash
# å®‰è£…/å‡çº§ ingress-nginx
helm upgrade --install ingress-nginx ingress-nginx-chart -n ingress-nginx

# æ£€æŸ¥ Pod çŠ¶æ€
kubectl get pod -n ingress-nginx
# é¢„æœŸè¾“å‡ºç¤ºä¾‹ï¼š
# NAME                                             READY   STATUS    RESTARTS   AGE
# ingress-nginx-controller-6c5cc57697-2pkrb        1/1     Running   0          109s
# ingress-nginx-controller-6c5cc57697-2wyjs        1/1     Running   0          88s
# ingress-nginx-controller-6c5cc57697-d1vsz        1/1     Running   0          78s

# æ£€æŸ¥ Service å’Œ Endpoint
kubectl get svc -n ingress-nginx
kubectl get ep -n ingress-nginx
```

##### 1.5.3 éªŒè¯ Ingress è®¿é—®

```bash
# è·å– Ingress IP å’ŒåŸŸåæ˜ å°„
kubectl get ingress --all-namespaces

# ä½¿ç”¨ curl éªŒè¯ç‰¹å®šåŸŸåçš„è®¿é—®
# ç¤ºä¾‹: curl --resolve demo.localdev.me:80:172.27.71.49 http://demo.localdev.me:80
# é¢„æœŸè¾“å‡º: <html><body><h1>works</h1></body></html>

# æ£€æŸ¥ Ingress Controller æ—¥å¿—
kubectl logs -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx

# éªŒè¯ SSL/TLS é…ç½®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
kubectl get secrets -n ingress-nginx
```

##### 1.5.4 æ•…éšœæ’æŸ¥æ¸…å•

```bash
# 1. æ£€æŸ¥ Ingress Controller Pod çŠ¶æ€
kubectl describe pod -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx

# 2. æ£€æŸ¥ Ingress é…ç½®
kubectl describe ingress -n <namespace> <ingress-name>

# 3. æ£€æŸ¥ Service åç«¯
kubectl get svc -n <namespace>
kubectl describe svc -n <namespace> <service-name>

# 4. æ£€æŸ¥ Endpoints
kubectl get endpoints -n <namespace>
```

### æ­¥éª¤2: Master èŠ‚ç‚¹å‡çº§ (1.27 => 1.28)

#### 2.0 å‡çº§æœŸé—´å®æ—¶ç›‘æ§

åœ¨æ•´ä¸ªå‡çº§è¿‡ç¨‹ä¸­ï¼Œéœ€è¦åœ¨å•ç‹¬çš„ç»ˆç«¯çª—å£ä¸­è¿è¡Œä»¥ä¸‹ç›‘æ§å‘½ä»¤ï¼Œå®æ—¶è§‚å¯Ÿé›†ç¾¤çŠ¶æ€ï¼š

```bash
# ç»ˆç«¯1: ç›‘æ§ Master/Control-Plane èŠ‚ç‚¹çŠ¶æ€
watch "kubectl get nodes | grep control"

# ç»ˆç«¯2: ç›‘æ§å…³é”®ç³»ç»Ÿç»„ä»¶ï¼ˆkube-system namespaceï¼‰
kubectl get pods -n kube-system -w | grep -E 'cluster|etcd|apiserver|scheduler|cloud|core|controller'

# ç»ˆç«¯3: ç›‘æ§æ‰€æœ‰å¼‚å¸¸ Pod
kubectl get pod -n kube-system -w | grep -vE 'Running|Completed'
```

**ç›‘æ§è¯´æ˜**ï¼š
- `watch` å‘½ä»¤ä¼šæ¯ 2 ç§’åˆ·æ–°ä¸€æ¬¡èŠ‚ç‚¹çŠ¶æ€
- `-w` å‚æ•°è¡¨ç¤ºæŒç»­ç›‘æ§ï¼ˆwatch modeï¼‰
- ä¿æŒè¿™äº›ç›‘æ§çª—å£æ‰“å¼€ï¼Œç›´åˆ°å‡çº§å®Œæˆ

#### 2.1 å‡çº§å‰æ£€æŸ¥

```bash
# å¤‡ä»½ etcd
kubectl get pods -n kube-system | grep etcd

# æ£€æŸ¥ master èŠ‚ç‚¹çŠ¶æ€
kubectl get nodes --selector=node-role.kubernetes.io/master

# æ£€æŸ¥å…³é”®ç³»ç»Ÿ Pod
kubectl get pods -n kube-system --field-selector=status.phase!=Running
```

#### 2.2 æ‰§è¡Œ Master å‡çº§

```bash
# æ’ç©º master èŠ‚ç‚¹ (å¦‚æœæœ‰å·¥ä½œè´Ÿè½½)
kubectl drain <master-node-name> --ignore-daemonsets --delete-emptydir-data

# å‡çº§ kubeadm
sudo apt-get update && sudo apt-get install -y kubeadm=1.28.x-00

# éªŒè¯å‡çº§è®¡åˆ’
sudo kubeadm upgrade plan

# æ‰§è¡Œå‡çº§
sudo kubeadm upgrade apply v1.28.x

# å‡çº§ kubelet å’Œ kubectl
sudo apt-get install -y kubelet=1.28.x-00 kubectl=1.28.x-00
sudo systemctl daemon-reload
sudo systemctl restart kubelet

# æ¢å¤èŠ‚ç‚¹è°ƒåº¦
kubectl uncordon <master-node-name>
```

### æ­¥éª¤2.5: Master èŠ‚ç‚¹å‡çº§ (1.28 => 1.29)

#### 2.5.1 å‡çº§å‰æ£€æŸ¥

```bash
# æ£€æŸ¥ master èŠ‚ç‚¹çŠ¶æ€
kubectl get nodes --selector=node-role.kubernetes.io/master

# æ£€æŸ¥å…³é”®ç³»ç»Ÿ Pod
kubectl get pods -n kube-system --field-selector=status.phase!=Running
```

#### 2.5.2 æ‰§è¡Œ Master å‡çº§

```bash
# æ’ç©º master èŠ‚ç‚¹ (å¦‚æœæœ‰å·¥ä½œè´Ÿè½½)
kubectl drain <master-node-name> --ignore-daemonsets --delete-emptydir-data

# å‡çº§ kubeadm
sudo apt-get update && sudo apt-get install -y kubeadm=1.29.x-00

# éªŒè¯å‡çº§è®¡åˆ’
sudo kubeadm upgrade plan

# æ‰§è¡Œå‡çº§
sudo kubeadm upgrade apply v1.29.x

# å‡çº§ kubelet å’Œ kubectl
sudo apt-get install -y kubelet=1.29.x-00 kubectl=1.29.x-00
sudo systemctl daemon-reload
sudo systemctl restart kubelet

# æ¢å¤èŠ‚ç‚¹è°ƒåº¦
kubectl uncordon <master-node-name>
```

### æ­¥éª¤3: ç»„ä»¶å‡çº§ (1.27 => 1.29)

#### 3.1 Calico å‡çº§

```bash
# æ£€æŸ¥ calico pods
kubectl get pods -n kube-system | grep calico

# éªŒè¯ pod é—´é€šä¿¡
kubectl exec -it <test-pod> -- ping <target-pod-ip>
```

#### 3.2 Cloud Provider å‡çº§

```bash
# æ£€æŸ¥ cloud-controller-manager
kubectl get pods -n kube-system | grep cloud-controller

# æ›´æ–° cloud provider é…ç½®
kubectl apply -f https://raw.githubusercontent.com/kubernetes/cloud-provider-aws/v1.29.x/manifests/rbac.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/cloud-provider-aws/v1.29.x/manifests/aws-cloud-controller-manager-daemonset.yaml

# éªŒè¯ LoadBalancer æœåŠ¡
kubectl get svc --all-namespaces | grep LoadBalancer
```

#### 3.3 Cluster Autoscaler å‡çº§

```bash
# æ£€æŸ¥å½“å‰ç‰ˆæœ¬
kubectl get deployment cluster-autoscaler -n kube-system -o yaml | grep image

# æ›´æ–° Cluster Autoscaler
kubectl set image deployment/cluster-autoscaler cluster-autoscaler=k8s.gcr.io/autoscaling/cluster-autoscaler:v1.29.x -n kube-system

# éªŒè¯æ–°èŠ‚ç‚¹åŠ å…¥åŠŸèƒ½
kubectl get nodes --watch
```

#### 3.4 Kube State Metrics å‡çº§

```bash
# æ£€æŸ¥å½“å‰ç‰ˆæœ¬
kubectl get deployment kube-state-metrics -n kube-system -o yaml | grep image

# æ›´æ–° kube-state-metrics
kubectl apply -f https://github.com/kubernetes/kube-state-metrics/examples/standard/

# éªŒè¯ç›‘æ§æ•°æ®
curl -s http://<kube-state-metrics-service>/metrics | head -10
```

**Grafana ç›‘æ§é¢æ¿**: [dev-pod-resources](https://grafana-mgt.dv-api.com/d/devasd_XlLjRMz/dev-pod-resources?orgId=1)

#### 3.5 Ingress Nginx Controller å‡çº§

```bash
# æ£€æŸ¥å½“å‰ç‰ˆæœ¬
kubectl get deployment ingress-nginx-controller -n ingress-nginx -o yaml | grep image

# æ›´æ–° Ingress Controller
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/aws/deploy.yaml

# éªŒè¯ Ingress åŠŸèƒ½
kubectl get ingress --all-namespaces
curl -I http://<your-ingress-domain>
```

### æ­¥éª¤4: Worker èŠ‚ç‚¹å‡çº§ (1.27 => 1.29)

#### 4.1 æ•°æ®åº“èŠ‚ç‚¹å‡çº§

```bash
# è¯†åˆ«æ•°æ®åº“æœåŠ¡èŠ‚ç‚¹
kubectl get pod -A -o wide | grep -E 'mysql|yb-|kafka|chi' | grep -o 'ip-.*internal' | sort | uniq -c

# é€ä¸ªå‡çº§æ•°æ®åº“èŠ‚ç‚¹
for node in $(kubectl get nodes --selector='!node-role.kubernetes.io/master' -o jsonpath='{.items[*].metadata.name}'); do
    echo "å‡çº§èŠ‚ç‚¹: $node"
    
    # æ’ç©ºèŠ‚ç‚¹
    kubectl drain $node --ignore-daemonsets --delete-emptydir-data --timeout=300s
    
    # SSH åˆ°èŠ‚ç‚¹æ‰§è¡Œå‡çº§
    ssh $node "
        sudo apt-get update
        sudo apt-get install -y kubeadm=1.29.x-00
        sudo kubeadm upgrade node
        sudo apt-get install -y kubelet=1.29.x-00 kubectl=1.29.x-00
        sudo systemctl daemon-reload
        sudo systemctl restart kubelet
    "
    
    # æ¢å¤èŠ‚ç‚¹è°ƒåº¦
    kubectl uncordon $node
    
    # ç­‰å¾…èŠ‚ç‚¹å°±ç»ª
    kubectl wait --for=condition=Ready node/$node --timeout=300s
    
    echo "èŠ‚ç‚¹ $node å‡çº§å®Œæˆ"
done
```

#### 4.2 éªŒè¯æ•°æ®åº“æœåŠ¡

```bash
# æ£€æŸ¥ MySQL æœåŠ¡
kubectl get pods -A | grep mysql
kubectl exec -it <mysql-pod> -n <namespace> -- mysql -u root -p -e "SELECT VERSION();"

# æ£€æŸ¥ Kafka æœåŠ¡
kubectl get pods -A | grep kafka
kubectl exec -it <kafka-pod> -n <namespace> -- /opt/kafka/bin/kafka-topics.sh --list --bootstrap-server localhost:9092

# æ£€æŸ¥ YugabyteDB æœåŠ¡
kubectl get pods -A | grep yb-
kubectl exec -it <yb-master-pod> -n <namespace> -- /home/yugabyte/bin/yb-admin list_all_masters

# æ£€æŸ¥ ClickHouse æœåŠ¡
kubectl get pods -A | grep chi
kubectl exec -it <clickhouse-pod> -n <namespace> -- clickhouse-client --query "SELECT version()"
```

### æ­¥éª¤5: å…¨é¢éªŒè¯æ£€æŸ¥

#### 5.1 åº”ç”¨ Pod çŠ¶æ€æ£€æŸ¥

```bash
# æ£€æŸ¥æ‰€æœ‰å‘½åç©ºé—´ Pod çŠ¶æ€
kubectl get pods --all-namespaces --field-selector=status.phase!=Running

# æ£€æŸ¥å…³é”®åº”ç”¨
kubectl get pods -n <critical-namespace>

# æ£€æŸ¥ Pod é‡å¯æ¬¡æ•°
kubectl get pods --all-namespaces --sort-by='.status.containerStatuses[0].restartCount' | tail -20
```

#### 5.2 é€šä¿¡éªŒè¯

```mermaid
graph LR
    A[Pod] -->|æµ‹è¯•| B[Pod]
    C[Node] -->|æµ‹è¯•| D[Pod]
    E[Pod] -->|æµ‹è¯•| F[å¤–éƒ¨æœåŠ¡]
    G[Ingress] -->|æµ‹è¯•| H[Service]
    
    subgraph "é€šä¿¡æµ‹è¯•è·¯å¾„"
        I[chi-pod] --> J[mysql-pod]
        K[node] --> L[chi-pod]
        M[chi-pod] --> N[å¤–éƒ¨API]
    end
```

```bash
# Pod åˆ° Pod é€šä¿¡æµ‹è¯• (chi â†’ mysql)
CHI_POD=$(kubectl get pods -A | grep chi | head -1 | awk '{print $2}')
MYSQL_POD=$(kubectl get pods -A | grep mysql | head -1 | awk '{print $2}')
kubectl exec -it $CHI_POD -- ping <mysql-pod-ip>

# Node åˆ° Pod é€šä¿¡æµ‹è¯•
NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}')
kubectl exec -it $CHI_POD -- ping $NODE_IP

# Pod åˆ°å¤–éƒ¨æœåŠ¡é€šä¿¡æµ‹è¯•
kubectl exec -it $CHI_POD -- curl -I https://www.google.com

# DNS è§£ææµ‹è¯•
kubectl exec -it $CHI_POD -- nslookup kubernetes.default.svc.cluster.local
```

#### 5.3 Ingress è®¿é—®éªŒè¯

```bash
# æ£€æŸ¥ Ingress èµ„æº
kubectl get ingress --all-namespaces

# æµ‹è¯• Ingress è®¿é—®
for ingress in $(kubectl get ingress --all-namespaces -o jsonpath='{.items[*].spec.rules[*].host}'); do
    echo "æµ‹è¯• Ingress: $ingress"
    curl -I http://$ingress
done

# æ£€æŸ¥è¯ä¹¦çŠ¶æ€
kubectl get certificates --all-namespaces
```

### æ­¥éª¤6: å‡çº§åæµé‡æ¢å¤ä¸éªŒè¯

#### 6.1 ç­‰å¾…é›†ç¾¤ç¨³å®š

å‡çº§å®Œæˆåï¼Œç­‰å¾… **5 åˆ†é’Ÿ**ï¼Œç¡®ä¿é›†ç¾¤å®Œå…¨ç¨³å®šåå†è¿›è¡Œæµé‡åˆ‡æ¢ã€‚

```bash
# ç­‰å¾…æœŸé—´æŒç»­ç›‘æ§
kubectl get pods --all-namespaces --field-selector=status.phase!=Running
kubectl get nodes
```

#### 6.2 æ¢å¤ MirrorMaker æ•°æ®åŒæ­¥

åœ¨å‡çº§å®Œ Cluster A åï¼Œæµé‡ä»åœ¨ Cluster Bï¼Œéœ€è¦å…ˆæ¢å¤ MirrorMaker è®©æ•°æ®ä» B åŒæ­¥åˆ° Aã€‚

**é‡è¦åŸåˆ™**ï¼šå…ˆå¯åŠ¨**æœ‰æµé‡çš„ä¸€ç«¯**çš„ MirrorMaker

```bash
# 1. å¯åŠ¨ B â†’ A çš„ MirrorMakerï¼ˆæœ‰æµé‡çš„ B å…ˆå¯åŠ¨ï¼‰
kubectl scale deployment mirrormaker-b-to-a --replicas=1 -n <namespace>

# 2. éªŒè¯ MirrorMaker Pod çŠ¶æ€
kubectl get pods -n <namespace> | grep mirrormaker

# 3. æ£€æŸ¥ MirrorMaker æ—¥å¿—
kubectl logs -f deployment/mirrormaker-b-to-a -n <namespace>
```

**ç›‘æ§ MirrorMaker Lag**ï¼š

è®¿é—® Grafana é¢æ¿è§‚å¯Ÿ B â†’ A çš„ lag å˜åŒ–ï¼š
- [MirrorLag ç›‘æ§](https://grafana-mgt.dv-api.com/d/-N7cUPZNk/mirrorlag-v2?orgId=1&var-cluster=aws-uswest2-prod-b&var-namespace=prod&var-source=cluster_b&var-target=cluster_a&var-topic=All)

**é¢„æœŸè¡Œä¸º**ï¼š
- **B â†’ A çš„ lag å¼€å§‹ä¸‹é™**ï¼ˆä¹‹å‰å‡çº§æœŸé—´é˜»å¡çš„æ•°æ®å¼€å§‹åŒæ­¥ï¼‰
- Lag é€æ¸é™ä½ï¼Œç›´è‡³æ¥è¿‘ 0
- è¿™è¡¨ç¤ºæ•°æ®æ­£åœ¨ä» B åŒæ­¥åˆ° A

#### 6.3 éªŒè¯ Kafka æ•°æ®æ¶ˆè´¹

æ£€æŸ¥ **Cluster A** çš„ Kafka æ˜¯å¦æ­£åœ¨æ¶ˆè´¹æ•°æ®ï¼ˆä»æ— åˆ°æœ‰ï¼Œå› ä¸ºæ•°æ®æ¥è‡ª Bï¼‰ï¼š

```bash
# 1. æ£€æŸ¥ Kafka Consumer Group çŠ¶æ€
kubectl exec -it <kafka-pod> -n <namespace> -- \
  /opt/kafka/bin/kafka-consumer-groups.sh \
  --bootstrap-server localhost:9092 \
  --describe --group <consumer-group-name>

# 2. æ£€æŸ¥ Topic çš„æ¶ˆè´¹ offset å˜åŒ–
kubectl exec -it <kafka-pod> -n <namespace> -- \
  /opt/kafka/bin/kafka-consumer-groups.sh \
  --bootstrap-server localhost:9092 \
  --describe --all-groups

# 3. ç›‘æ§å…³é”®ä¸šåŠ¡ Topic
# è§‚å¯Ÿä»¥ä¸‹æŒ‡æ ‡ï¼š
# - cluster: Cluster A
# - velocity: æ¶ˆè´¹é€Ÿåº¦ï¼ˆåº”è¯¥ä» 0 å¼€å§‹ä¸Šå‡ï¼‰
# - backfill: å›å¡«æ•°æ®é‡
```

**Grafana ç›‘æ§é¢æ¿**ï¼š
- æŸ¥çœ‹ Cluster A çš„ Kafka æ¶ˆè´¹æƒ…å†µ
- ç¡®è®¤æ•°æ®å¼€å§‹ä» B åŒæ­¥åˆ° A

#### 6.4 æµé‡åˆ‡æ¢å› Cluster A

åœ¨ç¡®è®¤ä»¥ä¸‹æ¡ä»¶åï¼Œå¯ä»¥åˆ‡æ¢æµé‡ï¼š

**åˆ‡æ¢å‰æ£€æŸ¥æ¸…å•**ï¼š
- âœ… Cluster A æ‰€æœ‰èŠ‚ç‚¹çŠ¶æ€ Ready
- âœ… Cluster A æ‰€æœ‰å…³é”® Pod çŠ¶æ€ Running
- âœ… MirrorMaker B â†’ A çš„ lag å·²é™è‡³ä½æ°´å¹³ï¼ˆ< 1000ï¼‰
- âœ… Kafka æ•°æ®åŒæ­¥æ­£å¸¸
- âœ… Ingress è®¿é—®æµ‹è¯•é€šè¿‡
- âœ… æ•°æ®åº“æœåŠ¡å¥åº·æ£€æŸ¥é€šè¿‡

```bash
# åˆ‡æ¢æµé‡å› Cluster A
# ï¼ˆé€šè¿‡ API Gateway/Load Balancer ç­‰æµé‡æ§åˆ¶æœºåˆ¶ï¼‰

# å®æ—¶ç›‘æ§æµé‡åˆ†å¸ƒ
```

**Grafana ç›‘æ§é¢æ¿**ï¼š
- [å¤šé›†ç¾¤æµé‡åˆ†å¸ƒç›‘æ§](https://grafana-mgt.dv-api.com/d/X2qhqpjSk/multi-cluster-traffic-distribution?orgId=1&var-cluster=aws-uswest2-prod&var-client=All&var-interface=All&refresh=5s)
- [SLA æ‰¹å¤„ç†å’Œå®æ—¶ç›‘æ§](https://grafana-mgt.dv-api.com/d/p1KqfRAMk/sla-batch-and-realtime?orgId=1&var-PromDs=vms-victoria-metrics-single-server&var-client=sofi&var-sandbox_client=airasia&var-pipeline=&var-Batch_Pipeline=prod.awsus&from=now-30m&to=now&refresh=5s)

#### 6.5 æ¢å¤åŒå‘ MirrorMaker

æµé‡åˆ‡å› A åï¼Œæ¢å¤åŒå‘ MirrorMaker åŒæ­¥ï¼š

```bash
# 1. å¯åŠ¨ A â†’ B çš„ MirrorMaker
kubectl scale deployment mirrormaker-a-to-b --replicas=1 -n <namespace>

# 2. éªŒè¯åŒå‘ MirrorMaker çŠ¶æ€
kubectl get pods -n <namespace> | grep mirrormaker

# 3. è§‚å¯ŸåŒå‘ lagï¼ˆA â†’ B å’Œ B â†’ A éƒ½åº”è¯¥å¾ˆä½ï¼‰
```

**é¢„æœŸç»“æœ**ï¼š
- A â†’ B lagï¼šå¹³ç¨³ä¸”ä½ï¼ˆå› ä¸º A ç°åœ¨æœ‰æµé‡ï¼Œå‘ B åŒæ­¥ï¼‰
- B â†’ A lagï¼šç»§ç»­ä¿æŒä½æ°´å¹³
- åŒå‘æ•°æ®åŒæ­¥æ­£å¸¸

#### 6.6 å…³é”®åº”ç”¨éªŒè¯

éªŒè¯ä»¥ä¸‹å…³é”®åº”ç”¨å’ŒæœåŠ¡ï¼š

```bash
# 1. JumpServer éªŒè¯
# è®¿é—® JumpServer ç®¡ç†ç•Œé¢
curl -I https://jumpserver.<your-domain>.com
# æ‰‹åŠ¨ç™»å½•éªŒè¯åŠŸèƒ½æ­£å¸¸

# 2. dcluster éªŒè¯
kubectl get pods -A | grep dcluster
kubectl logs -n <namespace> <dcluster-pod> --tail=50

# 3. ç‰¹å¾å¹³å°æœåŠ¡éªŒè¯
kubectl get pods -A | grep -E 'fp|ngsc|sdg|platform-api-server'
kubectl exec -it <fp-pod> -n <namespace> -- curl http://localhost:8080/health

# 4. æ•°æ®åº“æœåŠ¡éªŒè¯ï¼ˆå‚è€ƒæ­¥éª¤ 4.2ï¼‰
kubectl get pods -A | grep -E 'mysql|yb-|kafka|chi'

# 5. ç›‘æ§ç³»ç»ŸéªŒè¯
# æ£€æŸ¥ Grafanaã€Prometheusã€VictoriaMetrics ç­‰ç›‘æ§æœåŠ¡
kubectl get pods -n monitoring

# 6. æ—¥å¿—ç³»ç»ŸéªŒè¯
# æ£€æŸ¥ Lokiã€Elasticsearch ç­‰æ—¥å¿—æœåŠ¡
kubectl get pods -n logging
```

#### 6.7 æœ€ç»ˆéªŒè¯æ¸…å•

**å®Œæ•´çš„å‡çº§åéªŒè¯æ¸…å•**ï¼š

| æ£€æŸ¥é¡¹ | éªŒè¯å‘½ä»¤/æ–¹æ³• | é¢„æœŸç»“æœ | çŠ¶æ€ |
|--------|--------------|---------|------|
| èŠ‚ç‚¹ç‰ˆæœ¬ | `kubectl get nodes -o wide` | æ‰€æœ‰èŠ‚ç‚¹ç‰ˆæœ¬ä¸€è‡´ï¼ˆv1.29.xï¼‰ | â¬œ |
| Pod çŠ¶æ€ | `kubectl get pods -A \| grep -vE 'Running\|Completed'` | æ— å¼‚å¸¸ Pod | â¬œ |
| Master èŠ‚ç‚¹ | `kubectl get nodes \| grep control` | æ‰€æœ‰ Master Ready | â¬œ |
| ç³»ç»Ÿç»„ä»¶ | `kubectl get pods -n kube-system` | æ‰€æœ‰ç³»ç»Ÿ Pod Running | â¬œ |
| MirrorMaker | Grafana lag ç›‘æ§ | åŒå‘ lag < 1000 | â¬œ |
| Kafka æ¶ˆè´¹ | Kafka consumer group æ£€æŸ¥ | æ¶ˆè´¹æ­£å¸¸ï¼Œoffset å¢é•¿ | â¬œ |
| æµé‡åˆ†å¸ƒ | Grafana æµé‡ç›‘æ§ | æµé‡åœ¨ Cluster A | â¬œ |
| Ingress | `curl -I <ingress-domains>` | æ‰€æœ‰åŸŸåè®¿é—®æ­£å¸¸ | â¬œ |
| æ•°æ®åº“ | æ•°æ®åº“è¿æ¥æµ‹è¯• | MySQL/Kafka/YB/CH æ­£å¸¸ | â¬œ |
| JumpServer | ç™»å½•æµ‹è¯• | ç™»å½•å’ŒåŠŸèƒ½æ­£å¸¸ | â¬œ |
| dcluster | Pod å’Œæ—¥å¿—æ£€æŸ¥ | æœåŠ¡è¿è¡Œæ­£å¸¸ | â¬œ |
| ç›‘æ§ç³»ç»Ÿ | Grafana è®¿é—® | ç›‘æ§æ•°æ®æ­£å¸¸ | â¬œ |
| å‘Šè­¦ | æ£€æŸ¥å‘Šè­¦å¹³å° | æ— ä¸¥é‡å‘Šè­¦ | â¬œ |

**æœ€ç»ˆç¡®è®¤**ï¼š
```bash
# æ‰“å°å‡çº§å®ŒæˆæŠ¥å‘Š
echo "================================"
echo "Kubernetes é›†ç¾¤å‡çº§å®Œæˆ"
echo "================================"
echo "é›†ç¾¤ç‰ˆæœ¬: $(kubectl version --short)"
echo "èŠ‚ç‚¹æ•°é‡: $(kubectl get nodes --no-headers | wc -l)"
echo "Master èŠ‚ç‚¹æ•°: $(kubectl get nodes --selector=node-role.kubernetes.io/master --no-headers | wc -l)"
echo "Worker èŠ‚ç‚¹æ•°: $(kubectl get nodes --selector='!node-role.kubernetes.io/master' --no-headers | wc -l)"
echo "å¼‚å¸¸ Pod æ•°é‡: $(kubectl get pods -A --field-selector=status.phase!=Running,status.phase!=Succeeded --no-headers 2>/dev/null | wc -l)"
echo "================================"
```

## å…³é”®å‘½ä»¤æ±‡æ€»

### å‡çº§å‰å‡†å¤‡å‘½ä»¤

```bash
# é›†ç¾¤ä¿¡æ¯æ”¶é›†
kubectl version --short
kubectl get nodes -o wide
kubectl get pod -A | grep -i dcluster | awk '{print $1}' | sort | uniq -c | awk '{print $2}'

# ä¿å­˜é›†ç¾¤çŠ¶æ€
kubectl get pods --all-namespaces -o wide > cluster_pods_before_upgrade.txt
kubectl get endpoints --all-namespaces | grep -i none > endpoints_issues_before_upgrade.txt
kubectl get nodes -o wide > cluster_nodes_before_upgrade.txt

# æ•°æ®åº“æœåŠ¡èŠ‚ç‚¹æ£€æŸ¥
kubectl get pod -A -o wide | grep -E 'mysql|yb-|kafka|chi' | grep -o 'ip-.*internal' | sort | uniq -c

# MirrorMaker ç®¡ç†ï¼ˆå‡çº§ Cluster A å‰ï¼‰
kubectl scale deployment mirrormaker-a-to-b --replicas=0 -n <namespace>
kubectl scale deployment mirrormaker-b-to-a --replicas=0 -n <namespace>
```

### å‡çº§æœŸé—´å®æ—¶ç›‘æ§å‘½ä»¤

```bash
# ç›‘æ§ Master èŠ‚ç‚¹çŠ¶æ€
watch "kubectl get nodes | grep control"

# ç›‘æ§å…³é”®ç³»ç»Ÿç»„ä»¶
kubectl get pods -n kube-system -w | grep -E 'cluster|etcd|apiserver|scheduler|cloud|core|controller'

# ç›‘æ§å¼‚å¸¸ Pod
kubectl get pod -n kube-system -w | grep -vE 'Running|Completed'
```

### å‡çº§çŠ¶æ€æ£€æŸ¥å‘½ä»¤

```bash
# å‡çº§çŠ¶æ€æ£€æŸ¥
kubectl get pods --all-namespaces --field-selector=status.phase!=Running
kubectl get nodes --selector=node-role.kubernetes.io/master

# ç»„ä»¶çŠ¶æ€æ£€æŸ¥
kubectl get deployment cluster-autoscaler -n kube-system
kubectl get deployment kube-state-metrics -n kube-system  
kubectl get deployment ingress-nginx-controller -n ingress-nginx

# é€šä¿¡æµ‹è¯•
kubectl exec -it <pod-name> -- ping <target-ip>
kubectl exec -it <pod-name> -- curl -I <target-url>
kubectl exec -it <pod-name> -- nslookup <service-name>
```

### å‡çº§åéªŒè¯å‘½ä»¤

```bash
# æ¢å¤ MirrorMakerï¼ˆæœ‰æµé‡çš„ä¸€ç«¯å…ˆå¯åŠ¨ï¼‰
kubectl scale deployment mirrormaker-b-to-a --replicas=1 -n <namespace>
kubectl scale deployment mirrormaker-a-to-b --replicas=1 -n <namespace>

# Kafka æ¶ˆè´¹éªŒè¯
kubectl exec -it <kafka-pod> -n <namespace> -- \
  /opt/kafka/bin/kafka-consumer-groups.sh \
  --bootstrap-server localhost:9092 \
  --describe --all-groups

# å…³é”®åº”ç”¨éªŒè¯
kubectl get pods -A | grep -E 'jumpserver|dcluster|fp|ngsc|sdg'
kubectl get pods -A | grep -E 'mysql|yb-|kafka|chi'

# æœ€ç»ˆç‰ˆæœ¬ç¡®è®¤
kubectl get nodes -o wide | awk '{print $1, $5}'
```

## å›æ»šè®¡åˆ’

å¦‚æœå‡çº§è¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜ï¼Œæ‰§è¡Œä»¥ä¸‹å›æ»šæ­¥éª¤ï¼š

```bash
# å›æ»š Master èŠ‚ç‚¹
sudo kubeadm upgrade apply v1.27.x --force

# å›æ»š Worker èŠ‚ç‚¹
sudo apt-get install -y kubelet=1.27.x-00 kubectl=1.27.x-00
sudo systemctl daemon-reload
sudo systemctl restart kubelet

# å›æ»šç»„ä»¶ç‰ˆæœ¬
kubectl rollout undo deployment/<component-name> -n <namespace>
```

## æ³¨æ„äº‹é¡¹

1. **å¤‡ä»½é‡è¦æ•°æ®**: å‡çº§å‰åŠ¡å¿…å¤‡ä»½ etcd å’Œé‡è¦åº”ç”¨æ•°æ®
2. **åˆ†é˜¶æ®µå‡çº§**: å…ˆå‡çº§æµ‹è¯•ç¯å¢ƒï¼ŒéªŒè¯æ— è¯¯åå†å‡çº§ç”Ÿäº§ç¯å¢ƒ
3. **ç›‘æ§å‘Šè­¦**: å‡çº§è¿‡ç¨‹ä¸­å¯†åˆ‡å…³æ³¨ç›‘æ§å‘Šè­¦å’Œæ—¥å¿—
4. **å›æ»šå‡†å¤‡**: ç¡®ä¿å›æ»šæ–¹æ¡ˆå¯ç”¨ï¼Œå¿…è¦æ—¶å¿«é€Ÿå›æ»š
5. **ä¸šåŠ¡çª—å£**: åœ¨ä¸šåŠ¡ä½å³°æœŸæ‰§è¡Œå‡çº§æ“ä½œ 
```
1. db
    - æ‰€åœ¨node
2. service 
    - æ‰€åœ¨node
3. prod ns
    - æ‰€åœ¨node
4. ä¸šåŠ¡ns ï¼ˆservice + dbï¼‰
```
```sh


# get node 
kubectl get pod -A -o wide|grep -E 'mysql|yb-|kafka|chi'|grep -o 'ip-.*internal'|sort|uniq -c

kubectl get pod -A -o wide|grep -E 'fp|ngsc|sdg|platform-api-server'|grep -o 'ip-.*internal'|sort|uniq -c

kubectl get pod -A -o wide|grep -E 'dcluster'|grep -o 'ip-.*internal'|sort|uniq -c

# get ns
kubectl get pod -A -o wide | grep -E 'mysql|yb-|kafka|chi' | awk '{print $1}' | sort | uniq

# general:

keastmgt get nodes | wc -l
keastmgt get nodes | grep -vE 'control-plane'
keastmgt get nodes | grep -E 'control-plane'

keastmgt get pod -A -o wide | grep -vE 'Running|Completed'

# db:
keastmgt get pod -A -o wide | grep -E 'mysql|yb-|kafka|chi' | grep -o 'ip-.*internal' | sort | uniq -c
keastmgt get pod -A -o wide | grep -E 'mysql|yb-|kafka|chi' | awk '{print $1}' | sort | uniq

# business:
keastmgt get pod -A -o wide|grep -E 'fp|ngsc|sdg|platform-api-server'|grep -o 'ip-.*internal'|sort|uniq -c

keastmgt get pod -A -o wide|grep -E 'dcluster'|grep -o 'ip-.*internal'|sort|uniq -c

# kube-system:
keastmgt get pod -n kube-system -o wide | grep -vE 'Running|Completed'


---
aws autoscaling describe-auto-scaling-groups \
  --region us-east-1 \
  --query 'AutoScalingGroups[?contains(AutoScalingGroupName, `mgt`)].{Name:AutoScalingGroupName, Min:MinSize, Max:MaxSize, Desired:DesiredCapacity}' \
  --output table| cat 

aws ec2 describe-instances \
  --region us-east-1 \
  --instance-ids $(aws autoscaling describe-auto-scaling-groups \
    --region us-east-1 \
    --query 'AutoScalingGroups[?contains(AutoScalingGroupName, `mgt`)].Instances[].InstanceId' \
    --output text) \
  --query 'Reservations[].Instances[].{ID:InstanceId, PrivateIP:PrivateIpAddress, PublicIP:PublicIpAddress}' \
  --output table| cat 

aws autoscaling describe-auto-scaling-groups --region us-east-1 | grep -A5 -B5 "mgt" | grep AutoScalingGroupName


```

